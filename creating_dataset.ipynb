{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import torch, torchvision\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 225\n",
    "amount_images_in_new_dataset = 150\n",
    "batch_size = 32\n",
    "dataset_save_dir = \"encoded_data/data\"\n",
    "new_dataset_dir = \"landing_site_classification\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_path= os.path.join(\"mars_dataset\", \"meta_clean.json\")\n",
    "json_data = json.load(open(json_path))\n",
    "python_data = {}\n",
    "\n",
    "for data_point in json_data:\n",
    "    python_data[data_point] = json_data[data_point][\"title\"] + \" \" + json_data[data_point][\"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fan in Southern Highlands Crater Many channels formed by water (when the climate of Mars was very different to that of today) cut the ancient highlands of Mars. Water running through these channels picks up rocky debris and carries it or rolls it along the channel bed. Occasionally these channels will empty into a crater or other low point in the terrain and the water will drop the material it is transporting. This material can build up in large fan-shaped mounds at the end of the channel. In this observation, this is likely what has happened. The fan-shaped mound (which appears bluish in this false-color image) sits at the end of a short channel. Analysis of spectroscopic data shows that the composition of this material indicates a history of interaction with liquid water. The full resolution version of this HiRISE image shows layering that indicates this material was dumped here in at least three separate episodes. Although they may once have been common, features like this are now rare. Scientists study them to try and understand how much liquid water affected the composition and appearance of Mars in its early history. Written by: Eldar Noe (2 September 2009)This is a stereo pair with ESP_020106_1670.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_data[\"ESP_014159_1670\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "class BertEncodedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict, model=model):\n",
    "        self.classes = [name for name in data_dict]\n",
    "        if not os.path.exists(dataset_save_dir):\n",
    "            text = [model.encode(text, convert_to_tensor=True) for text in list(data_dict.values())]\n",
    "            data = torch.stack(text)\n",
    "            os.makedirs(dataset_save_dir.split(\"/\")[0], exist_ok=True)\n",
    "            # y = torch.arange(len(data))\n",
    "            np.savez(dataset_save_dir, data.cpu())\n",
    "        npzfile = np.load(dataset_save_dir + \".npz\")\n",
    "        self.encoded = torch.from_numpy(npzfile['arr_0'])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.encoded)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.encoded[index], self.classes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BertEncodedDataset(python_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "query = model.encode(\"flat, water, equator\", convert_to_tensor=True)\n",
    "\n",
    "similar_dict = {}\n",
    "\n",
    "for desc, img in dataset:\n",
    "    similar_val = cos(query.to(device), desc.to(device))\n",
    "    similar_dict[similar_val.item()] = img\n",
    "\n",
    "sorted_dict = dict(sorted(similar_dict.items(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_site_images = list(sorted_dict.values())[:amount_images_in_new_dataset]\n",
    "not_landing_site_images = random.choices(list(sorted_dict.values())[amount_images_in_new_dataset:], k=amount_images_in_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(new_dataset_dir):\n",
    "    os.makedirs(new_dataset_dir)\n",
    "    landing_site_path = new_dataset_dir + \"/landing_site\"\n",
    "    not_landing_site_path = new_dataset_dir + \"/not_landing_site\"\n",
    "    os.makedirs(landing_site_path)\n",
    "    os.makedirs(not_landing_site_path)\n",
    "    for file_name in landing_site_images:\n",
    "        img = Image.open(f\"mars_dataset/images/{file_name}.jpg\")\n",
    "        img.save(landing_site_path + f\"/{file_name}.jpg\")  \n",
    "    for file_name in not_landing_site_images:\n",
    "        img = Image.open(f\"mars_dataset/images/{file_name}.jpg\")\n",
    "        img.save(not_landing_site_path + f\"/{file_name}.jpg\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "model.conv1 = nn.Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(2048, 17)\n",
    "model.load_state_dict(torch.load(\"resnet50-sentinel2.pt\"))\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = nn.Linear(2048, 1)\n",
    "transform = ResNet50_Weights.DEFAULT.transforms()\n",
    "\n",
    "### Try using a model that has 10 input channels, try loading a model from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(new_dataset_dir, transform=transform)\n",
    "n_test = int(0.15 * len(dataset))  # take ~10% for test\n",
    "test_set = torch.utils.data.Subset(dataset, range(n_test))  # take first 10%\n",
    "train_set = torch.utils.data.Subset(dataset, range(n_test, len(dataset))) \n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heloo\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\dev\\AI\\mars_image_explorer\\creating_dataset.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                        train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                        test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                        epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/dev/AI/mars_image_explorer/creating_dataset.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                        device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32md:\\dev\\AI\\mars_image_explorer\\trainer.py:171\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m--> 171\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    172\u001b[0m                                       dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[0;32m    173\u001b[0m                                       loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m    174\u001b[0m                                       optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m    175\u001b[0m                                       device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    176\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m    177\u001b[0m       dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[0;32m    178\u001b[0m       loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[0;32m    179\u001b[0m       device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    181\u001b[0m     \u001b[39m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[1;32md:\\dev\\AI\\mars_image_explorer\\trainer.py:49\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     48\u001b[0m \u001b[39m# 2. Calculate  and accumulate lossdada\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     50\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \n\u001b[0;32m     52\u001b[0m \u001b[39m# 3. Optimizer zero grad\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ossih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ossih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[0;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[0;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\ossih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3165\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m   3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m-> 3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "results = trainer.train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=5,\n",
    "                       device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
